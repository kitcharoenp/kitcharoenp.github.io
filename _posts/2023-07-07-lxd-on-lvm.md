---
layout : post
title : "Install LXD on LVM storage pool"
categories : [lxd]
published : false
---
### [Thinly-Provisioned Logical Volumes](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/logical_volume_manager_administration/thinly_provisioned_volume_creation)

> Logical volumes can be thinly provisioned. This allows you to create logical volumes that are larger than the available extents. Using thin provisioning, you can manage a storage pool of free space, known as a thin pool, which can be allocated to an arbitrary number of devices when needed by applications. 

create a test new `LXDPool` logical volume  `-t`
   * type : `thin-pool`  
   * thinpool-name : `LXDPool`
   * size : `100%FREE`
   * on volume group : `ubuntu-vg`

```shell
$ sudo lvcreate -t --type thin-pool --thinpool LXDPool -l 100%FREE ubuntu-vg
  TEST MODE: Metadata will NOT be updated and volumes will not be (de)activated.
  Thin pool volume with chunk size 512.00 KiB can address at most 126.50 TiB of data.
  WARNING: Pool zeroing and 512.00 KiB large chunk size slows down thin provisioning.
  WARNING: Consider disabling zeroing (-Zn) or using smaller chunk size (<512.00 KiB).
  Logical volume "LXDPool" created.
```

create a test new `LXDPool` logical volume  `-t` with `chunksize` is 128KiB

```shell
$ sudo lvcreate -t --type thin-pool --chunksize 128K --thinpool LXDPool -l 100%FREE ubuntu-vg
  TEST MODE: Metadata will NOT be updated and volumes will not be (de)activated.
  Thin pool volume with chunk size 128.00 KiB can address at most 31.62 TiB of data.
  Logical volume "LXDPool" created.
```
Remove the `-t` switch (for test) from the previous command to **actually increase the logical volume.**

### Display Logical volume

```shell
$ sudo lvdisplay 
  --- Logical volume ---
  LV Path                /dev/ubuntu-vg/ubuntu-lv
  LV Name                ubuntu-lv
  VG Name                ubuntu-vg
  LV UUID                xP6AsV-2A5E-npbO-LV5v-1DoJ-mZZ7-gIgYCO
  LV Write Access        read/write
  LV Creation host, time ubuntu-server, 2023-07-05 07:44:40 +0000
  LV Status              available
  # open                 1
  LV Size                265.69 GiB
  Current LE             68017
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0
   
  --- Logical volume ---
  LV Name                LXDPool
  VG Name                ubuntu-vg
  LV UUID                ZZpvpd-jfsd-NZbz-4f5v-CGcC-cf8g-l3xUij
  LV Write Access        read/write
  LV Creation host, time server03, 2023-07-12 09:07:30 +0000
  LV Pool metadata       LXDPool_tmeta
  LV Pool data           LXDPool_tdata
  LV Status              available
  # open                 0
  LV Size                <662.12 GiB
  Allocated pool data    0.00%
  Allocated metadata     5.23%
  Current LE             169502
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     512
  Block device           253:3
```

### Remove thinpool
```shell
$ sudo lvremove /dev/ubuntu-vg/LXDPool
Do you really want to remove and DISCARD active logical volume ubuntu-vg/LXDPool? [y/n]: y
  Logical volume "LXDPool" successfully removed

```


### LXD
```shell
$ lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: yes
What IP address or DNS name should be used to reach this server? [default=10.10.10.13]: 
Are you joining an existing cluster? (yes/no) [default=no]: no
What member name should be used to identify this server in the cluster? [default=server03]: 
Setup password authentication on the cluster? (yes/no) [default=no]: 
Do you want to configure a new local storage pool? (yes/no) [default=yes]: no
Do you want to configure a new remote storage pool? (yes/no) [default=no]: no
Would you like to connect to a MAAS server? (yes/no) [default=no]: 
Would you like to configure LXD to use an existing bridge or host interface? (yes/no) [default=no]: 
Would you like to create a new Fan overlay network? (yes/no) [default=yes]: 
What subnet should be used as the Fan underlay? [default=auto]: 10.10.10.0/24
Would you like stale cached images to be updated automatically? (yes/no) [default=yes]: 
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]: yes
config:
  core.https_address: 10.10.10.13:8443
networks:
- config:
    bridge.mode: fan
    fan.underlay_subnet: 10.10.10.0/24
  description: ""
  name: lxdfan0
  type: ""
  project: default
storage_pools: []
profiles:
- config: {}
  description: ""
  devices:
    eth0:
      name: eth0
      network: lxdfan0
      type: nic
  name: default
projects: []
cluster:
  server_name: server03
  enabled: true
  member_config: []
  cluster_address: ""
  cluster_certificate: ""
  server_address: ""
  cluster_password: ""
  cluster_certificate_path: ""
  cluster_token: ""

```

### Create lvm storage from the existing volume group 
```shell
$ lxc storage create pool lvm source=<ExistingVgName> lvm.vg.force_reuse=true lvm.use_thinpool=true lvm.thinpool_name=LXDPool
```

```shell
$ lxc storage create pool lvm source=ubuntu-vg lvm.vg.force_reuse=true lvm.use_thinpool=true lvm.thinpool_name=LXDPool
Storage pool pool created

```

This created a proper thin pool `LXDPool` in my existing `ubuntu-vg` volume group, instead of some loop to a file.


### Display logical volume

```shell
$ sudo lvdisplay 
  --- Logical volume ---
  LV Path                /dev/ubuntu-vg/ubuntu-lv
  LV Name                ubuntu-lv
  VG Name                ubuntu-vg
  LV UUID                i6G8bw-ILoA-Gpvb-afUb-fQZm-rOHM-o6apw5
  LV Write Access        read/write
  LV Creation host, time ubuntu-server, 2023-07-18 07:15:21 +0000
  LV Status              available
  # open                 1
  LV Size                200.00 GiB
  Current LE             51200
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0
   
  --- Logical volume ---
  LV Name                LXDPool
  VG Name                ubuntu-vg
  LV UUID                TCb3Gt-ktI4-Dkkn-XOV8-K4Ck-oBFf-EdSN0Q
  LV Write Access        read/write
  LV Creation host, time server03, 2023-07-18 11:54:42 +0000
  LV Pool metadata       LXDPool_tmeta
  LV Pool data           LXDPool_tdata
  LV Status              available
  # open                 0
  LV Size                364.05 GiB
  Allocated pool data    0.00%
  Allocated metadata     10.43%
  Current LE             93197
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     1024
  Block device           253:3
   
$ lxc storage list
+------+--------+-------------+---------+---------+
| NAME | DRIVER | DESCRIPTION | USED BY |  STATE  |
+------+--------+-------------+---------+---------+
| pool | lvm    |             | 0       | CREATED |
```

### Add `pool` lvm storage to the `default` profile 

```shell
$ lxc profile device add default root disk path=/ pool=pool
```

snap set lxd lvm.external=true and then reboot (this will ensure that the LVM metadata cache is in sync on the host and in the snap).

[For the lvm driver](https://documentation.ubuntu.com/lxd/en/latest/howto/storage_backup_volume/#use-snapshots-for-backup), snapshot creation is quick, but restoring snapshots is efficient only when using thin-pool mode.


Note that by default LXD will create a [loop device](https://stanislas.blog/2018/02/lxc-zfs-pool-lxd/) for you ZFS pool, which means weâ€™re using ZFS over our existing filesystem.

### Reference
* [LXD on LVM - thinpool setup](https://www.pither.com/simon/blog/2018/09/28/lxd-lvm-thinpool-setup)

* [How to create a default storage pool on LVM?](https://discuss.linuxcontainers.org/t/how-to-create-a-default-storage-pool-on-lvm/1735)

* [Setup a ZFS pool for your LXC containers with LXD](https://stanislas.blog/2018/02/lxc-zfs-pool-lxd/)

* [How to setup LXD on Ubuntu 20.04 with whole rootfs already in ZFS](https://discuss.linuxcontainers.org/t/how-to-setup-lxd-on-ubuntu-20-04-with-whole-rootfs-already-in-zfs/7199)

rpool/lxd